{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3. Medical text categorization\n",
    "Thi, Le Thanh - 1504521 <br>\n",
    "Cognitive Systems for Health Technology Applications <br>\n",
    "Helsinki Metropolia University of Applied Sciences <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "To use recurrent and convolutional neural networks to create a classifier for a collection of medical abstracts extracted from MEDLINE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Preprocessing Data\n",
    "http://disi.unitn.it/moschitti/corpora.htm. Download than pre-computed embedding from the link. Cardiovascular diseases abstracts, file name 'ohsumed-first-20000-docs.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ohsumed-first-20000-docs.tar.gz', <http.client.HTTPMessage at 0x1400c45b550>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "url = r\"http://disi.unitn.it/moschitti/corpora/ohsumed-first-20000-docs.tar.gz\"\n",
    "dst = 'ohsumed-first-20000-docs.tar.gz'\n",
    "urlretrieve(url, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract the tarfile. Creates a folder: ohsu-trec\n",
    "import tarfile\n",
    "tar = tarfile.open(\"ohsumed-first-20000-docs.tar.gz\")\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "Collect the individual training review into a list of strings, on string per review and let's also collect the review labels into a lables list. So we have 23 Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ohsumed_dir = r'C:\\Users\\Thi\\Desktop\\ohsumed-first-20000-docs'\n",
    "train_dir = os.path.join(ohsumed_dir, 'training')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "# Generate a list L = ['C01', 'C02, ..., 'C023']\n",
    "L = ['C{:02}'.format(n) for n in range(1, 23 + 1)]\n",
    "for label_type in L:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        f = open(os.path.join(dir_name, fname))\n",
    "        texts.append(f.read())\n",
    "        f.close()\n",
    "        i = L.index(label_type)\n",
    "        labels.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tokenize the text of the raw data\n",
    "Vectorize the texts have been collected. We will review after 500 words and only consider the top 20,000 words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30857 unique tokens\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 500        \n",
    "max_words = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (10433, 500)\n",
      "Shape of labels tensor: (10433, 23)\n"
     ]
    }
   ],
   "source": [
    "#Shape the data and labels\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "data = pad_sequences(sequences, maxlen = maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "one_hot_labels = to_categorical(labels)\n",
    "\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of labels tensor:', one_hot_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shuffle the data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "one_hot_labels = one_hot_labels[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Build the model\n",
    "Make a simple network with random layer and number of neurals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 8)            160000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              4097024   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 23)                5911      \n",
      "=================================================================\n",
      "Total params: 4,525,335\n",
      "Trainable params: 4,525,335\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 8, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation = 'relu'))\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dense(len(L), activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8346 samples, validate on 2087 samples\n",
      "Epoch 1/10\n",
      "8346/8346 [==============================] - 58s 7ms/step - loss: 2.7404 - acc: 0.1973 - val_loss: 2.6500 - val_acc: 0.2329\n",
      "Epoch 2/10\n",
      "8346/8346 [==============================] - 56s 7ms/step - loss: 2.4171 - acc: 0.2816 - val_loss: 2.6038 - val_acc: 0.2544\n",
      "Epoch 3/10\n",
      "8346/8346 [==============================] - 56s 7ms/step - loss: 1.9807 - acc: 0.3699 - val_loss: 2.8503 - val_acc: 0.1787\n",
      "Epoch 4/10\n",
      "8346/8346 [==============================] - 55s 7ms/step - loss: 1.5952 - acc: 0.4752 - val_loss: 3.0187 - val_acc: 0.1485\n",
      "Epoch 5/10\n",
      "8346/8346 [==============================] - 55s 7ms/step - loss: 1.3420 - acc: 0.5087 - val_loss: 3.2955 - val_acc: 0.1346\n",
      "Epoch 6/10\n",
      "8346/8346 [==============================] - 54s 6ms/step - loss: 1.1688 - acc: 0.5270 - val_loss: 3.6446 - val_acc: 0.1332\n",
      "Epoch 7/10\n",
      "8346/8346 [==============================] - 52s 6ms/step - loss: 1.0450 - acc: 0.5369 - val_loss: 3.8777 - val_acc: 0.1375\n",
      "Epoch 8/10\n",
      "8346/8346 [==============================] - 55s 7ms/step - loss: 0.9408 - acc: 0.5377 - val_loss: 4.1947 - val_acc: 0.1346\n",
      "Epoch 9/10\n",
      "8346/8346 [==============================] - 55s 7ms/step - loss: 0.8625 - acc: 0.5461 - val_loss: 4.7157 - val_acc: 0.1265\n",
      "Epoch 10/10\n",
      "8346/8346 [==============================] - 55s 7ms/step - loss: 0.7958 - acc: 0.5532 - val_loss: 5.0776 - val_acc: 0.1332\n",
      "Elapsed time: 552.07 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "history = model.fit(data, one_hot_labels,\n",
    "                   epochs = 10,\n",
    "                   batch_size = 32,\n",
    "                   validation_split = 0.2)\n",
    "t2 = time.time()\n",
    "print('Elapsed time: {:.2f} seconds'.format((t2-t1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA20AAADgCAYAAACQNI5RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1401506b6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2QAAADgCAYAAABsIaWbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14015c363c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Metrics results\n",
    "\n",
    "But looking the result above the highest accuracy we can get iss 25% at the second epoch. But to the classification, accuracy alone is a bad measure for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the predictions\n",
    "predictions = model.predict(data)\n",
    "y_pred = np.argmax(predictions, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[241  10   1  13   7  15   4  18   2   3   8   4  13  15   1   5   8   0   0  16   9   2  28]\n",
      " [ 11  61   0  15   0   6   2   6   4   3   3   2   5   5   1   1   2   0   1  24   0   0   6]\n",
      " [  3   1  35   2   0   2   0   3   0   2   2   1   0   2   0   1   0   2   0   3   0   1   5]\n",
      " [ 10   4   1 886   8  15  20  41  17   7  11  23  14   6   3   5  12   2  21   7   3   0  47]\n",
      " [ 12   0   0  12 165   3   1   3   0   4   2   2   2   4   2   7   7   5   2   7  17   0  26]\n",
      " [ 26   5   3  77   4 286   0   9   0   3   1   7   6  26   2   4   8   8   8   6   6   7  86]\n",
      " [  2   1   0  10   3   1  72   1   2   0   0   1   0   2   0   0   1   2   0   0   0   0   2]\n",
      " [ 23   6   3  43   3   6   2 249  12   1   1   1   2  31   1   8   2   4   2  20  18   4  31]\n",
      " [  0   0   0  13   2   0   0   7  80   3   2   1   1   1   0   0   1   1   0   2   1   0  10]\n",
      " [ 11   3   2  45  24   7   4   4   2 239  16   4   8  51   0   7   6   6   8  13  32   4 125]\n",
      " [  1   1   2   7   4   1   1   0   0   0 108   2   0   3   0   3   4   3   0   6   2   0  14]\n",
      " [ 15   0   0  94   1   8   0  11   1   5   0 228   9  33   2   6   4   9  12   4   2   2  45]\n",
      " [  9   5   1  37   0   5   0   6   0   1   0   4 167   4   1   2   6   1   2  11   0   0  19]\n",
      " [ 18   1   1  26   4   4   2  12   0  14   7  15  18 954   3  22  13  19   8   3  14   3  88]\n",
      " [ 10   2   1  27   7   4   2  13   1   1   3  15   9  13  64   2   7   3   2  11   3   1  14]\n",
      " [  3   0   0  12   4   1   1   3   2   3   0   1   9  19   0 125   1   1   1   3   1   0  10]\n",
      " [  4   2   0  28  20   1   7   3   4   4   6   5   3   4   0   4 162   0   3  14   2   2  17]\n",
      " [  3   1   0  18   5   7   2   2   0   2   4  11   2  38   0   1   2 224  23   2   5   1  35]\n",
      " [  0   0   0  19   6   1   0   1   0   2   3   1   0  20   0   1   2  20 101   4   1   0   9]\n",
      " [ 20  17   2  54   6  12   4  30   6   8   5  11   8  11   2   1  12   3   5 280  11   1  16]\n",
      " [ 17   0   1  19  22  18   2   5   2   3   8   6   2  25   2   0  12   5   1  12 328   7  49]\n",
      " [  6   1   0   2   3   5   0   2   0   1   0   0   0  21   0   0   4   3   1   1   2  37   3]\n",
      " [ 53  12   6 193  49  90  13  62  24  20  23  37  33 284  11  39  32  14  15  33  48   8 700]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(labels, y_pred)\n",
    "print(np.array2string(cm, max_line_width = 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading confusion matrix\n",
    "We have 23 class, so our matrix 23x23. For example, in class/label 0, text belong to file C01, out of 498 prediction only 241 are correct. This ratio is precision, we can see it in below table. Another ratio is recall, 57% in class 0. It means in 423 text or data belong to class 0. The model only picks up 241. Combie both ratio we have f1-score to measure our model. The average score is 55%              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.57      0.52       423\n",
      "          1       0.46      0.39      0.42       158\n",
      "          2       0.59      0.54      0.56        65\n",
      "          3       0.54      0.76      0.63      1163\n",
      "          4       0.48      0.58      0.52       283\n",
      "          5       0.57      0.49      0.53       588\n",
      "          6       0.52      0.72      0.60       100\n",
      "          7       0.51      0.53      0.52       473\n",
      "          8       0.50      0.64      0.56       125\n",
      "          9       0.73      0.38      0.50       621\n",
      "         10       0.51      0.67      0.58       162\n",
      "         11       0.60      0.46      0.52       491\n",
      "         12       0.54      0.59      0.56       281\n",
      "         13       0.61      0.76      0.68      1249\n",
      "         14       0.67      0.30      0.41       215\n",
      "         15       0.51      0.62      0.56       200\n",
      "         16       0.53      0.55      0.54       295\n",
      "         17       0.67      0.58      0.62       388\n",
      "         18       0.47      0.53      0.50       191\n",
      "         19       0.58      0.53      0.56       525\n",
      "         20       0.65      0.60      0.62       546\n",
      "         21       0.46      0.40      0.43        92\n",
      "         22       0.51      0.39      0.44      1799\n",
      "\n",
      "avg / total       0.56      0.56      0.55     10433\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification results per class\n",
    "print(classification_report(labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to improve performance\n",
    "By add more tool, changing the capacity of network to get a better result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Require Libraries\n",
    "from keras.layers import LSTM,  Dropout, Conv1D, MaxPooling1D, GlobalMaxPooling1D, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           640000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 492, 32)           9248      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 90, 32)            9248      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                6240      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              33792     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 23)                23575     \n",
      "=================================================================\n",
      "Total params: 722,103\n",
      "Trainable params: 722,103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 32, input_length=maxlen))\n",
    "model.add(Conv1D(32, 9, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(32, 9, activation='relu'))\n",
    "model.add(GRU(32, dropout=0.1, recurrent_dropout=0.5))\n",
    "model.add(Dense(1024, activation = 'relu'))\n",
    "model.add(Dense(len(L), activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "I add a convnet layer after embedding word, increase dimension and use the rule of thumbs. The rule is if N is output. The number of neural in hidden layer will in ration of ...3xN 2xN 1.5xN N\n",
    "GRU in place of Flatten, LSTM or GlobalMaxPooling.  <br>\n",
    "\n",
    "The result: parameter is reduce from 4,5 million to around 7 hundred thousand. The validation accuracy is poorly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8346 samples, validate on 2087 samples\n",
      "Epoch 1/12\n",
      "8346/8346 [==============================] - 62s 7ms/step - loss: 2.8802 - acc: 0.1517 - val_loss: 2.8993 - val_acc: 0.1260\n",
      "Epoch 2/12\n",
      "8346/8346 [==============================] - 61s 7ms/step - loss: 2.8328 - acc: 0.1511 - val_loss: 2.9058 - val_acc: 0.1188\n",
      "Epoch 3/12\n",
      "8346/8346 [==============================] - 66s 8ms/step - loss: 2.7751 - acc: 0.1745 - val_loss: 2.8755 - val_acc: 0.1893\n",
      "Epoch 4/12\n",
      "8346/8346 [==============================] - 62s 7ms/step - loss: 2.7961 - acc: 0.1742 - val_loss: 2.9464 - val_acc: 0.1514\n",
      "Epoch 5/12\n",
      "8346/8346 [==============================] - 60s 7ms/step - loss: 2.8355 - acc: 0.1842 - val_loss: 2.9466 - val_acc: 0.1998\n",
      "Epoch 6/12\n",
      "8346/8346 [==============================] - 64s 8ms/step - loss: 2.8873 - acc: 0.1964 - val_loss: 3.5905 - val_acc: 0.1893\n",
      "Epoch 7/12\n",
      "8346/8346 [==============================] - 62s 7ms/step - loss: 2.9915 - acc: 0.1825 - val_loss: 3.5039 - val_acc: 0.1030\n",
      "Epoch 8/12\n",
      "8346/8346 [==============================] - 56s 7ms/step - loss: 3.1240 - acc: 0.1783 - val_loss: 3.4315 - val_acc: 0.1926\n",
      "Epoch 9/12\n",
      "8346/8346 [==============================] - 58s 7ms/step - loss: 3.2366 - acc: 0.1746 - val_loss: 3.8641 - val_acc: 0.1327\n",
      "Epoch 10/12\n",
      "8346/8346 [==============================] - 60s 7ms/step - loss: 3.3589 - acc: 0.1782 - val_loss: 3.9219 - val_acc: 0.1581\n",
      "Epoch 11/12\n",
      "8346/8346 [==============================] - 58s 7ms/step - loss: 3.5194 - acc: 0.1816 - val_loss: 5.9114 - val_acc: 0.0379\n",
      "Epoch 12/12\n",
      "8346/8346 [==============================] - 55s 7ms/step - loss: 3.5893 - acc: 0.1879 - val_loss: 4.4951 - val_acc: 0.1169\n",
      "Elapsed time: 728.18 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "history = model.fit(data, one_hot_labels,\n",
    "                   epochs = 12,\n",
    "                   batch_size = 32,\n",
    "                   validation_split = 0.2)\n",
    "t2 = time.time()\n",
    "print('Elapsed time: {:.2f} seconds'.format((t2-t1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "Compare to the first model. It does better in the later epochs so I thought perhap if I introduce to resisting overfitting tools, it could do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_31 (Embedding)     (None, 500, 46)           920000    \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 23000)             0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 690)               15870690  \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 690)               0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 460)               317860    \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 460)               0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 230)               106030    \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 230)               0         \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 23)                5313      \n",
      "=================================================================\n",
      "Total params: 17,219,893\n",
      "Trainable params: 17,219,893\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 46, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(690,kernel_regularizer=regularizers.l2(0.009), activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(460,kernel_regularizer=regularizers.l2(0.009), activation = 'relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(230,kernel_regularizer=regularizers.l2(0.009), activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(L), activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "The capacity of the model have been increase and also I add two way to reduce overfitting, droptou and regulazier. <br>\n",
    "\n",
    "The result: The losses have been increase but the accuracies still nearly the same even after 30 epochs, but it does resist the overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8346 samples, validate on 2087 samples\n",
      "Epoch 1/30\n",
      "8346/8346 [==============================] - 66s 8ms/step - loss: 6.1785 - acc: 0.1540 - val_loss: 3.5134 - val_acc: 0.1730\n",
      "Epoch 2/30\n",
      "8346/8346 [==============================] - 62s 7ms/step - loss: 3.1463 - acc: 0.1619 - val_loss: 2.9485 - val_acc: 0.1730\n",
      "Epoch 3/30\n",
      "8346/8346 [==============================] - 61s 7ms/step - loss: 2.8716 - acc: 0.1783 - val_loss: 2.8595 - val_acc: 0.1969\n",
      "Epoch 4/30\n",
      "8346/8346 [==============================] - 61s 7ms/step - loss: 2.7765 - acc: 0.1945 - val_loss: 2.9264 - val_acc: 0.1931\n",
      "Epoch 5/30\n",
      "8346/8346 [==============================] - 61s 7ms/step - loss: 2.7253 - acc: 0.2006 - val_loss: 2.8743 - val_acc: 0.1854\n",
      "Epoch 6/30\n",
      "8346/8346 [==============================] - 60s 7ms/step - loss: 2.7001 - acc: 0.2160 - val_loss: 2.8572 - val_acc: 0.1869\n",
      "Epoch 7/30\n",
      "8346/8346 [==============================] - 61s 7ms/step - loss: 2.6663 - acc: 0.2171 - val_loss: 2.8434 - val_acc: 0.1907\n",
      "Epoch 8/30\n",
      "8346/8346 [==============================] - 61s 7ms/step - loss: 2.6478 - acc: 0.2268 - val_loss: 2.8589 - val_acc: 0.2012\n",
      "Epoch 9/30\n",
      "8346/8346 [==============================] - 61s 7ms/step - loss: 2.6305 - acc: 0.2281 - val_loss: 2.9004 - val_acc: 0.1993\n",
      "Epoch 10/30\n",
      "8346/8346 [==============================] - 62s 7ms/step - loss: 2.6061 - acc: 0.2362 - val_loss: 2.9077 - val_acc: 0.1624\n",
      "Epoch 11/30\n",
      "8346/8346 [==============================] - 61s 7ms/step - loss: 2.5929 - acc: 0.2384 - val_loss: 2.9276 - val_acc: 0.1974\n",
      "Epoch 12/30\n",
      "8346/8346 [==============================] - 61s 7ms/step - loss: 2.5789 - acc: 0.2428 - val_loss: 3.1237 - val_acc: 0.2161\n",
      "Epoch 13/30\n",
      "8346/8346 [==============================] - 61s 7ms/step - loss: 2.5742 - acc: 0.2429 - val_loss: 2.9498 - val_acc: 0.1984\n",
      "Epoch 14/30\n",
      "8346/8346 [==============================] - 60s 7ms/step - loss: 2.5593 - acc: 0.2486 - val_loss: 2.9536 - val_acc: 0.1552\n",
      "Epoch 15/30\n",
      "8346/8346 [==============================] - 62s 7ms/step - loss: 2.5535 - acc: 0.2508 - val_loss: 2.9635 - val_acc: 0.1993\n",
      "Epoch 16/30\n",
      "8346/8346 [==============================] - 61s 7ms/step - loss: 2.5422 - acc: 0.2513 - val_loss: 2.9453 - val_acc: 0.1744\n",
      "Epoch 17/30\n",
      "8346/8346 [==============================] - 62s 7ms/step - loss: 2.5356 - acc: 0.2588 - val_loss: 3.0182 - val_acc: 0.2012\n",
      "Epoch 18/30\n",
      "8346/8346 [==============================] - 61s 7ms/step - loss: 2.5252 - acc: 0.2679 - val_loss: 2.9976 - val_acc: 0.1500\n",
      "Epoch 19/30\n",
      "8346/8346 [==============================] - 61s 7ms/step - loss: 2.5238 - acc: 0.2652 - val_loss: 3.3301 - val_acc: 0.2084\n",
      "Epoch 20/30\n",
      "8346/8346 [==============================] - 61s 7ms/step - loss: 2.5219 - acc: 0.2683 - val_loss: 3.0735 - val_acc: 0.2022\n",
      "Epoch 21/30\n",
      "8346/8346 [==============================] - 61s 7ms/step - loss: 2.5152 - acc: 0.2653 - val_loss: 3.0140 - val_acc: 0.1735\n",
      "Epoch 22/30\n",
      "8346/8346 [==============================] - 61s 7ms/step - loss: 2.5064 - acc: 0.2721 - val_loss: 3.0546 - val_acc: 0.1782\n",
      "Epoch 23/30\n",
      "8346/8346 [==============================] - 61s 7ms/step - loss: 2.5013 - acc: 0.2735 - val_loss: 3.2644 - val_acc: 0.2084\n",
      "Epoch 24/30\n",
      "8346/8346 [==============================] - 61s 7ms/step - loss: 2.5011 - acc: 0.2725 - val_loss: 3.0370 - val_acc: 0.1653\n",
      "Epoch 25/30\n",
      "8346/8346 [==============================] - 62s 7ms/step - loss: 2.5023 - acc: 0.2750 - val_loss: 3.1098 - val_acc: 0.1878\n",
      "Epoch 26/30\n",
      "8346/8346 [==============================] - 61s 7ms/step - loss: 2.4912 - acc: 0.2750 - val_loss: 3.0391 - val_acc: 0.1610\n",
      "Epoch 27/30\n",
      "8346/8346 [==============================] - 62s 7ms/step - loss: 2.4825 - acc: 0.2771 - val_loss: 3.1289 - val_acc: 0.1945\n",
      "Epoch 28/30\n",
      "8346/8346 [==============================] - 62s 7ms/step - loss: 2.4834 - acc: 0.2739 - val_loss: 3.3599 - val_acc: 0.2103\n",
      "Epoch 29/30\n",
      "8346/8346 [==============================] - 62s 7ms/step - loss: 2.4829 - acc: 0.2807 - val_loss: 3.1437 - val_acc: 0.1883\n",
      "Epoch 30/30\n",
      "8346/8346 [==============================] - 61s 7ms/step - loss: 2.4811 - acc: 0.2791 - val_loss: 3.0804 - val_acc: 0.1634\n",
      "Elapsed time: 1841.97 seconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "history = model.fit(data, one_hot_labels,\n",
    "                   epochs = 30,\n",
    "                   batch_size = 128,\n",
    "                   validation_split = 0.2)\n",
    "t2 = time.time()\n",
    "print('Elapsed time: {:.2f} seconds'.format((t2-t1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_34 (Embedding)     (None, 500, 32)           640000    \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 492, 32)           9248      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 90, 32)            9248      \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 32)                6240      \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 1024)              33792     \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 23)                23575     \n",
      "=================================================================\n",
      "Total params: 722,103\n",
      "Trainable params: 722,103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 32, input_length=maxlen))\n",
    "model.add(Conv1D(32, 9, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(32, 9, activation='relu'))\n",
    "model.add(GRU(32, dropout=0.1, recurrent_dropout=0.5))\n",
    "model.add(Dense(1024, activation = 'relu'))\n",
    "model.add(Dense(len(L), activation = 'softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adamax', \n",
    "              metrics= ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "The best performance in train and validate but this's just a nonsense model, created after many anttempt to improve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8346 samples, validate on 2087 samples\n",
      "Epoch 1/10\n",
      "8346/8346 [==============================] - 31s 4ms/step - loss: 0.1653 - acc: 0.9565 - val_loss: 0.1631 - val_acc: 0.9565\n",
      "Epoch 2/10\n",
      "8346/8346 [==============================] - 26s 3ms/step - loss: 0.1619 - acc: 0.9565 - val_loss: 0.1615 - val_acc: 0.9565\n",
      "Epoch 3/10\n",
      "8346/8346 [==============================] - 26s 3ms/step - loss: 0.1576 - acc: 0.9566 - val_loss: 0.1607 - val_acc: 0.9566\n",
      "Epoch 4/10\n",
      "8346/8346 [==============================] - 26s 3ms/step - loss: 0.1533 - acc: 0.9569 - val_loss: 0.1613 - val_acc: 0.9565\n",
      "Epoch 5/10\n",
      "8346/8346 [==============================] - 26s 3ms/step - loss: 0.1497 - acc: 0.9570 - val_loss: 0.1611 - val_acc: 0.9565\n",
      "Epoch 6/10\n",
      "8346/8346 [==============================] - 26s 3ms/step - loss: 0.1467 - acc: 0.9574 - val_loss: 0.1637 - val_acc: 0.9566\n",
      "Epoch 7/10\n",
      "8346/8346 [==============================] - 26s 3ms/step - loss: 0.1444 - acc: 0.9576 - val_loss: 0.1644 - val_acc: 0.9564\n",
      "Epoch 8/10\n",
      "8346/8346 [==============================] - 27s 3ms/step - loss: 0.1427 - acc: 0.9579 - val_loss: 0.1684 - val_acc: 0.9564\n",
      "Epoch 9/10\n",
      "8346/8346 [==============================] - 26s 3ms/step - loss: 0.1403 - acc: 0.9582 - val_loss: 0.1703 - val_acc: 0.9561\n",
      "Epoch 10/10\n",
      "8346/8346 [==============================] - 26s 3ms/step - loss: 0.1391 - acc: 0.9583 - val_loss: 0.1717 - val_acc: 0.9560\n",
      "Elapsed time: 269.06 seconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "history = model.fit(data, one_hot_labels,\n",
    "                   epochs = 10,\n",
    "                   batch_size = 128,\n",
    "                   validation_split = 0.2)\n",
    "t2 = time.time()\n",
    "print('Elapsed time: {:.2f} seconds'.format((t2-t1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_36 (Embedding)     (None, 500, 32)           640000    \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 492, 32)           9248      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 90, 32)            9248      \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 32)                6240      \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 1024)              33792     \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 23)                23575     \n",
      "=================================================================\n",
      "Total params: 722,103\n",
      "Trainable params: 722,103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.metrics import categorical_accuracy\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 32, input_length=maxlen))\n",
    "model.add(Conv1D(32, 9, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(32, 9, activation='relu'))\n",
    "model.add(GRU(32, dropout=0.1, recurrent_dropout=0.5))\n",
    "model.add(Dense(1024, activation = 'relu'))\n",
    "model.add(Dense(len(L), activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = [categorical_accuracy])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "Last try and still fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8346 samples, validate on 2087 samples\n",
      "Epoch 1/10\n",
      "8346/8346 [==============================] - 30s 4ms/step - loss: 2.8232 - categorical_accuracy: 0.1674 - val_loss: 2.7949 - val_categorical_accuracy: 0.1730\n",
      "Epoch 2/10\n",
      "8346/8346 [==============================] - 25s 3ms/step - loss: 2.7035 - categorical_accuracy: 0.1751 - val_loss: 2.6963 - val_categorical_accuracy: 0.2065\n",
      "Epoch 3/10\n",
      "8346/8346 [==============================] - 26s 3ms/step - loss: 2.5764 - categorical_accuracy: 0.2025 - val_loss: 2.8164 - val_categorical_accuracy: 0.2113\n",
      "Epoch 4/10\n",
      "8346/8346 [==============================] - 26s 3ms/step - loss: 2.5005 - categorical_accuracy: 0.2213 - val_loss: 2.7194 - val_categorical_accuracy: 0.2084\n",
      "Epoch 5/10\n",
      "8346/8346 [==============================] - 26s 3ms/step - loss: 2.4247 - categorical_accuracy: 0.2505 - val_loss: 2.7581 - val_categorical_accuracy: 0.2276\n",
      "Epoch 6/10\n",
      "8346/8346 [==============================] - 26s 3ms/step - loss: 2.3287 - categorical_accuracy: 0.2810 - val_loss: 2.7948 - val_categorical_accuracy: 0.2123\n",
      "Epoch 7/10\n",
      "8346/8346 [==============================] - 26s 3ms/step - loss: 2.2461 - categorical_accuracy: 0.2994 - val_loss: 2.8336 - val_categorical_accuracy: 0.2262\n",
      "Epoch 8/10\n",
      "8346/8346 [==============================] - 27s 3ms/step - loss: 2.1643 - categorical_accuracy: 0.3229 - val_loss: 3.0127 - val_categorical_accuracy: 0.2137\n",
      "Epoch 9/10\n",
      "8346/8346 [==============================] - 27s 3ms/step - loss: 2.0885 - categorical_accuracy: 0.3379 - val_loss: 2.9886 - val_categorical_accuracy: 0.1931\n",
      "Epoch 10/10\n",
      "8346/8346 [==============================] - 26s 3ms/step - loss: 2.0305 - categorical_accuracy: 0.3569 - val_loss: 3.0541 - val_categorical_accuracy: 0.1888\n",
      "Elapsed time: 266.94 seconds\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "history = model.fit(data, one_hot_labels,\n",
    "                   epochs = 10,\n",
    "                   batch_size = 128,\n",
    "                   validation_split = 0.2)\n",
    "t2 = time.time()\n",
    "print('Elapsed time: {:.2f} seconds'.format((t2-t1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion\n",
    "After the first model witout convet still give the best performance. Reading test is quite hard cause text is reading by human using natural language and so far right now. Human only can \"communicate\" with computer through programming language. I think the first step try to improve this task preprocessing, before tolkenize and embedding. A filter to clean the text before hand, drop out all the word with unnecessary or bring confusion to the machine.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
